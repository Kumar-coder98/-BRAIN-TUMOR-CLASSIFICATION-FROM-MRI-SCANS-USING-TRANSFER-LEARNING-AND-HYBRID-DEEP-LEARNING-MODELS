{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "40c206ecfd3544599b817363f90bb08d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e81c9d22596f4c40ad384ba414211b37",
              "IPY_MODEL_847e772b15d5450fbfdd0b84ee84b72f",
              "IPY_MODEL_44d93259c03c481abf6ba70f8b309197"
            ],
            "layout": "IPY_MODEL_4933b3b84a2d404cac185e78785b6b8a"
          }
        },
        "e81c9d22596f4c40ad384ba414211b37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_383957d8537747c4bc420adec6cc859b",
            "placeholder": "​",
            "style": "IPY_MODEL_af50bba9752246fc9b18cc0ea9de8ace",
            "value": "config.json: 100%"
          }
        },
        "847e772b15d5450fbfdd0b84ee84b72f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ad318024717499a89ad765cdae29c26",
            "max": 502,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_305b88964b7a4ed696398beb442a4ea5",
            "value": 502
          }
        },
        "44d93259c03c481abf6ba70f8b309197": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da5f86c75888426d9bda68866c1d3307",
            "placeholder": "​",
            "style": "IPY_MODEL_15af139400234d6c8837920975fc2cef",
            "value": " 502/502 [00:00&lt;00:00, 20.8kB/s]"
          }
        },
        "4933b3b84a2d404cac185e78785b6b8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "383957d8537747c4bc420adec6cc859b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af50bba9752246fc9b18cc0ea9de8ace": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ad318024717499a89ad765cdae29c26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "305b88964b7a4ed696398beb442a4ea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da5f86c75888426d9bda68866c1d3307": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15af139400234d6c8837920975fc2cef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e21aeaa2e1614c7b9b63f9de00f8d278": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b3cdf621f404d8a939e2a2c24e655a1",
              "IPY_MODEL_d7c50e061e46498886fc14631f91be43",
              "IPY_MODEL_4e3d13a666534a9aa174697a9ea255cc"
            ],
            "layout": "IPY_MODEL_2c50024d6de34082b75ae622303f6468"
          }
        },
        "9b3cdf621f404d8a939e2a2c24e655a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31b02f2114c7447894d2bfe96ab9e47c",
            "placeholder": "​",
            "style": "IPY_MODEL_473e9d2acd5e4533a47189b79a14eea3",
            "value": "model.safetensors: 100%"
          }
        },
        "d7c50e061e46498886fc14631f91be43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f27b74e0003c4c51b541e88664a20c5f",
            "max": 345579424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4afb9c0514748c3b1ff85456c75e2e3",
            "value": 345579424
          }
        },
        "4e3d13a666534a9aa174697a9ea255cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52f7468f6d504815ab0e5f007e37e6e2",
            "placeholder": "​",
            "style": "IPY_MODEL_c25c7e0f87c14ab7b2c59521a8335d4a",
            "value": " 346M/346M [00:05&lt;00:00, 26.8MB/s]"
          }
        },
        "2c50024d6de34082b75ae622303f6468": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31b02f2114c7447894d2bfe96ab9e47c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "473e9d2acd5e4533a47189b79a14eea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f27b74e0003c4c51b541e88664a20c5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4afb9c0514748c3b1ff85456c75e2e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "52f7468f6d504815ab0e5f007e37e6e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c25c7e0f87c14ab7b2c59521a8335d4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8Ub4FdYqL9j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"/content/sample_data/archive.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/sample_data\")"
      ],
      "metadata": {
        "id": "bsSikEtxr6wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ResNet152+Adam optimizer +(no PCA+NO SVM)\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "from torchvision import datasets, transforms, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# 1. Transform + Augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 2. Load dataset\n",
        "dataset = datasets.ImageFolder(root=\"/content/sample_data/Train\", transform=transform)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_data, val_data = random_split(dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=32)\n",
        "\n",
        "# 3. Load ResNet152 and fine-tune\n",
        "model = models.resnet152(pretrained=True)\n",
        "\n",
        "# Freeze early layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze only layer4 and fc\n",
        "for param in model.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Replace the fully connected layer\n",
        "num_classes = len(dataset.classes)\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(model.fc.in_features, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(512, num_classes)\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# 4. Define Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "# 5. Training Loop\n",
        "epochs = 10\n",
        "epoch_accuracies = []  # List to store accuracy for each epoch\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Calculate epoch accuracy\n",
        "    epoch_accuracy = correct / total\n",
        "    epoch_accuracies.append(epoch_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
        "\n",
        "# 6. Validation\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "# Calculate final average accuracy on validation set\n",
        "final_accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "print(f\"Final Validation Accuracy: {final_accuracy:.4f}\")\n",
        "\n",
        "# 7. Evaluation Report\n",
        "print(\"Classification Report:\\n\", classification_report(all_labels, all_preds))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(all_labels, all_preds))\n",
        "\n",
        "# 8. Predict on New Images\n",
        "pred_folder_path = \"/content/sample_data/pred\"\n",
        "# Transform same as training, but without augmentation (use only Resize, CenterCrop, ToTensor, Normalize)\n",
        "pred_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "model.eval()\n",
        "class_names = dataset.classes  # e.g., ['no', 'yes']\n",
        "print(f\"\\nClass Names: {class_names}\")\n",
        "print(\"\\nPredictions on /pred folder:\")\n",
        "for img_name in os.listdir(pred_folder_path):\n",
        "    img_path = os.path.join(pred_folder_path, img_name)\n",
        "    if os.path.isfile(img_path):\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')  # Ensure image is RGB\n",
        "            img_tensor = pred_transform(img).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                output = model(img_tensor)\n",
        "                _, pred = torch.max(output, 1)\n",
        "                label = class_names[pred.item()]\n",
        "            print(f\"{img_name} → Predicted: {label}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not process image {img_name}: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHGYu4Jv8n-k",
        "outputId": "640f2762-188f-442d-9a5e-19ca5ed658a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n",
            "100%|██████████| 230M/230M [00:02<00:00, 118MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.1940, Accuracy: 0.9213\n",
            "Epoch 2/10, Loss: 0.0537, Accuracy: 0.9812\n",
            "Epoch 3/10, Loss: 0.0355, Accuracy: 0.9879\n",
            "Epoch 4/10, Loss: 0.0201, Accuracy: 0.9946\n",
            "Epoch 5/10, Loss: 0.0160, Accuracy: 0.9954\n",
            "Epoch 6/10, Loss: 0.0113, Accuracy: 0.9954\n",
            "Epoch 7/10, Loss: 0.0084, Accuracy: 0.9979\n",
            "Epoch 8/10, Loss: 0.0091, Accuracy: 0.9971\n",
            "Epoch 9/10, Loss: 0.0082, Accuracy: 0.9967\n",
            "Epoch 10/10, Loss: 0.0027, Accuracy: 0.9992\n",
            "Final Validation Accuracy: 0.9933\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99       302\n",
            "           1       1.00      0.99      0.99       298\n",
            "\n",
            "    accuracy                           0.99       600\n",
            "   macro avg       0.99      0.99      0.99       600\n",
            "weighted avg       0.99      0.99      0.99       600\n",
            "\n",
            "Confusion Matrix:\n",
            " [[301   1]\n",
            " [  3 295]]\n",
            "\n",
            "Class Names: ['no', 'yes']\n",
            "\n",
            "Predictions on /pred folder:\n",
            "pred10.jpg → Predicted: no\n",
            "pred15.jpg → Predicted: no\n",
            "pred34.jpg → Predicted: yes\n",
            "pred23.jpg → Predicted: yes\n",
            "pred42.jpg → Predicted: no\n",
            "pred29.jpg → Predicted: no\n",
            "pred21.jpg → Predicted: no\n",
            "pred50.jpg → Predicted: no\n",
            "pred27.jpg → Predicted: no\n",
            "pred8.jpg → Predicted: yes\n",
            "pred2.jpg → Predicted: yes\n",
            "pred45.jpg → Predicted: yes\n",
            "pred16.jpg → Predicted: yes\n",
            "pred12.jpg → Predicted: no\n",
            "pred11.jpg → Predicted: yes\n",
            "pred48.jpg → Predicted: no\n",
            "pred41.jpg → Predicted: no\n",
            "pred56.jpg → Predicted: yes\n",
            "pred6.jpg → Predicted: yes\n",
            "pred32.jpg → Predicted: no\n",
            "pred58.jpg → Predicted: no\n",
            "pred59.jpg → Predicted: no\n",
            "pred33.jpg → Predicted: no\n",
            "pred36.jpg → Predicted: no\n",
            "pred24.jpg → Predicted: no\n",
            "pred39.jpg → Predicted: no\n",
            "pred18.jpg → Predicted: no\n",
            "pred55.jpg → Predicted: no\n",
            "pred52.jpg → Predicted: no\n",
            "pred3.jpg → Predicted: no\n",
            "pred9.jpg → Predicted: yes\n",
            "pred43.jpg → Predicted: no\n",
            "pred47.jpg → Predicted: no\n",
            "pred20.jpg → Predicted: no\n",
            "pred19.jpg → Predicted: no\n",
            "pred51.jpg → Predicted: no\n",
            "pred54.jpg → Predicted: no\n",
            "pred5.jpg → Predicted: yes\n",
            "pred44.jpg → Predicted: no\n",
            "pred35.jpg → Predicted: no\n",
            "pred22.jpg → Predicted: no\n",
            "pred0.jpg → Predicted: no\n",
            "pred38.jpg → Predicted: no\n",
            "pred30.jpg → Predicted: no\n",
            "pred26.jpg → Predicted: yes\n",
            "pred14.jpg → Predicted: yes\n",
            "pred4.jpg → Predicted: no\n",
            "pred40.jpg → Predicted: no\n",
            "pred28.jpg → Predicted: no\n",
            "pred7.jpg → Predicted: yes\n",
            "pred46.jpg → Predicted: no\n",
            "pred1.jpg → Predicted: no\n",
            "pred31.jpg → Predicted: no\n",
            "pred53.jpg → Predicted: no\n",
            "pred13.jpg → Predicted: yes\n",
            "pred37.jpg → Predicted: no\n",
            "pred57.jpg → Predicted: no\n",
            "pred49.jpg → Predicted: no\n",
            "pred17.jpg → Predicted: no\n",
            "pred25.jpg → Predicted: no\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ResNet152+SVM+PCA\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms, models\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# 1. Transform (no data augmentation needed — ResNet will extract features)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 2. Load dataset\n",
        "dataset = datasets.ImageFolder(root=\"/content/sample_data/Train\", transform=transform)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_data, val_data = random_split(dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=False)\n",
        "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
        "\n",
        "# 3. Load ResNet152 feature extractor (no classifier)\n",
        "resnet = models.resnet152(pretrained=True)\n",
        "feature_extractor = nn.Sequential(*list(resnet.children())[:-1])  # Remove FC layer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "feature_extractor = feature_extractor.to(device)\n",
        "feature_extractor.eval()\n",
        "\n",
        "# 4. Extract features\n",
        "def extract_features(loader):\n",
        "    features = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, lbls in loader:\n",
        "            images = images.to(device)\n",
        "            out = feature_extractor(images)  # Shape: (batch, 2048, 1, 1)\n",
        "            out = out.view(out.size(0), -1)  # Shape: (batch, 2048)\n",
        "            features.append(out.cpu().numpy())\n",
        "            labels.extend(lbls.numpy())\n",
        "    features = np.vstack(features)\n",
        "    labels = np.array(labels)\n",
        "    return features, labels\n",
        "\n",
        "# 5. Get features\n",
        "train_features, train_labels = extract_features(train_loader)\n",
        "val_features, val_labels = extract_features(val_loader)\n",
        "\n",
        "print(f\"Train features shape: {train_features.shape}\")\n",
        "print(f\"Val features shape: {val_features.shape}\")\n",
        "\n",
        "# 6. PCA + SVM pipeline\n",
        "pca = PCA(n_components=0.97, svd_solver='full')\n",
        "svm = SVC(kernel='rbf', C=1.0, probability=True)\n",
        "\n",
        "# Pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', pca),\n",
        "    ('svm', svm)\n",
        "])\n",
        "\n",
        "# 7. Train pipeline\n",
        "pipeline.fit(train_features, train_labels)\n",
        "\n",
        "# 8. Validation\n",
        "val_preds = pipeline.predict(val_features)\n",
        "# Calculate final average accuracy on validation set\n",
        "final_accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "print(f\"Final Validation Accuracy: {final_accuracy:.4f}\")\n",
        "# 9. Evaluation report\n",
        "print(\"Classification Report:\\n\", classification_report(val_labels, val_preds))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(val_labels, val_preds))\n",
        "\n",
        "# 10. Predict on new images\n",
        "pred_folder_path = \"/content/sample_data/pred\"\n",
        "pred_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "class_names = dataset.classes\n",
        "print(f\"\\nClass Names: {class_names}\")\n",
        "print(\"\\nPredictions on /pred folder:\")\n",
        "\n",
        "for img_name in os.listdir(pred_folder_path):\n",
        "    img_path = os.path.join(pred_folder_path, img_name)\n",
        "    if os.path.isfile(img_path):\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            img_tensor = pred_transform(img).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                out = feature_extractor(img_tensor)\n",
        "                out = out.view(1, -1).cpu().numpy()\n",
        "                # Apply PCA + SVM pipeline\n",
        "                pred = pipeline.predict(out)[0]\n",
        "                label = class_names[pred]\n",
        "            print(f\"{img_name} → Predicted: {label}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not process image {img_name}: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7Ya2wH2sM31",
        "outputId": "cd91eb54-3e56-4705-bac4-5034ea463f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train features shape: (2400, 2048)\n",
            "Val features shape: (600, 2048)\n",
            "Final Validation Accuracy: 0.9950\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       289\n",
            "           1       0.99      0.99      0.99       311\n",
            "\n",
            "    accuracy                           0.99       600\n",
            "   macro avg       0.99      0.99      0.99       600\n",
            "weighted avg       0.99      0.99      0.99       600\n",
            "\n",
            "Confusion Matrix:\n",
            " [[285   4]\n",
            " [  4 307]]\n",
            "\n",
            "Class Names: ['no', 'yes']\n",
            "\n",
            "Predictions on /pred folder:\n",
            "pred8.jpg → Predicted: yes\n",
            "pred6.jpg → Predicted: yes\n",
            "pred1.jpg → Predicted: no\n",
            "pred31.jpg → Predicted: no\n",
            "pred10.jpg → Predicted: yes\n",
            "pred2.jpg → Predicted: yes\n",
            "pred42.jpg → Predicted: no\n",
            "pred51.jpg → Predicted: no\n",
            "pred40.jpg → Predicted: no\n",
            "pred3.jpg → Predicted: no\n",
            "pred13.jpg → Predicted: yes\n",
            "pred29.jpg → Predicted: no\n",
            "pred38.jpg → Predicted: no\n",
            "pred22.jpg → Predicted: yes\n",
            "pred56.jpg → Predicted: yes\n",
            "pred49.jpg → Predicted: no\n",
            "pred9.jpg → Predicted: yes\n",
            "pred39.jpg → Predicted: yes\n",
            "pred30.jpg → Predicted: yes\n",
            "pred53.jpg → Predicted: no\n",
            "pred50.jpg → Predicted: no\n",
            "pred15.jpg → Predicted: no\n",
            "pred57.jpg → Predicted: no\n",
            "pred55.jpg → Predicted: yes\n",
            "pred54.jpg → Predicted: no\n",
            "pred45.jpg → Predicted: yes\n",
            "pred46.jpg → Predicted: no\n",
            "pred14.jpg → Predicted: yes\n",
            "pred23.jpg → Predicted: yes\n",
            "pred32.jpg → Predicted: no\n",
            "pred21.jpg → Predicted: no\n",
            "pred48.jpg → Predicted: no\n",
            "pred17.jpg → Predicted: no\n",
            "pred28.jpg → Predicted: yes\n",
            "pred19.jpg → Predicted: no\n",
            "pred37.jpg → Predicted: no\n",
            "pred24.jpg → Predicted: no\n",
            "pred12.jpg → Predicted: no\n",
            "pred16.jpg → Predicted: yes\n",
            "pred20.jpg → Predicted: no\n",
            "pred58.jpg → Predicted: no\n",
            "pred11.jpg → Predicted: yes\n",
            "pred27.jpg → Predicted: no\n",
            "pred44.jpg → Predicted: no\n",
            "pred35.jpg → Predicted: no\n",
            "pred59.jpg → Predicted: no\n",
            "pred0.jpg → Predicted: no\n",
            "pred7.jpg → Predicted: yes\n",
            "pred25.jpg → Predicted: no\n",
            "pred4.jpg → Predicted: no\n",
            "pred43.jpg → Predicted: no\n",
            "pred52.jpg → Predicted: no\n",
            "pred18.jpg → Predicted: no\n",
            "pred26.jpg → Predicted: yes\n",
            "pred41.jpg → Predicted: no\n",
            "pred36.jpg → Predicted: no\n",
            "pred33.jpg → Predicted: no\n",
            "pred5.jpg → Predicted: yes\n",
            "pred34.jpg → Predicted: yes\n",
            "pred47.jpg → Predicted: no\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#efficientB0 + PCA(99) +SVM\n",
        "import torch\n",
        "from torchvision import models, transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Dataset\n",
        "dataset = datasets.ImageFolder(root=\"/content/sample_data/Train\", transform=transform)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_data, val_data = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=False)\n",
        "val_loader = DataLoader(val_data, batch_size=32)\n",
        "\n",
        "# Load EfficientNetB0\n",
        "model = models.efficientnet_b0(pretrained=True)\n",
        "model.classifier = torch.nn.Identity()  # Remove final FC layer → output will be 1280-dim features\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Helper: extract features\n",
        "def extract_features(loader):\n",
        "    features = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, lbls in tqdm(loader):\n",
        "            images = images.to(device)\n",
        "            feats = model(images)  # output shape: (batch_size, 1280)\n",
        "            features.append(feats.cpu().numpy())\n",
        "            labels.append(lbls.numpy())\n",
        "    features = np.vstack(features)\n",
        "    labels = np.concatenate(labels)\n",
        "    return features, labels\n",
        "\n",
        "# Extract train/val features\n",
        "print(\"Extracting Train Features...\")\n",
        "train_features, train_labels = extract_features(train_loader)\n",
        "\n",
        "print(\"Extracting Validation Features...\")\n",
        "val_features, val_labels = extract_features(val_loader)\n",
        "\n",
        "# Pipeline: scaler → PCA(99%) → SVM (RBF kernel)\n",
        "pca = PCA(n_components=0.99, svd_solver='full')\n",
        "\n",
        "svm = SVC(kernel='rbf', C=10, gamma=0.001, probability=True)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', pca),\n",
        "    ('svm', svm)\n",
        "])\n",
        "\n",
        "# GridSearchCV to tune C, gamma\n",
        "param_grid = {\n",
        "    'svm__C': [10, 50, 100],\n",
        "    'svm__gamma': [0.001, 0.005, 0.01]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=3)\n",
        "\n",
        "# Fit\n",
        "print(\"Training SVM with GridSearch...\")\n",
        "grid.fit(train_features, train_labels)\n",
        "\n",
        "# Results\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "print(\"Best CV accuracy:\", grid.best_score_)\n",
        "\n",
        "# Evaluate on val set\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "val_preds = best_model.predict(val_features)\n",
        "\n",
        "print(\"Validation Classification Report:\\n\", classification_report(val_labels, val_preds))\n",
        "print(\"Validation Confusion Matrix:\\n\", confusion_matrix(val_labels, val_preds))\n",
        "\n",
        "# Predict on new images\n",
        "print(\"\\nPredicting on /pred folder...\")\n",
        "\n",
        "# pred folder transform (no augmentation)\n",
        "pred_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "pred_folder_path = \"/content/sample_data/pred\"\n",
        "class_names = dataset.classes\n",
        "\n",
        "for img_name in os.listdir(pred_folder_path):\n",
        "    img_path = os.path.join(pred_folder_path, img_name)\n",
        "    if os.path.isfile(img_path):\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            img_tensor = pred_transform(img).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                feat = model(img_tensor)\n",
        "            feat_np = feat.cpu().numpy()\n",
        "            pred_label = best_model.predict(feat_np)[0]\n",
        "            print(f\"{img_name} → Predicted: {class_names[pred_label]}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not process {img_name}: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YzFmGRftr-u",
        "outputId": "4c9b4d34-eb0c-4a8d-c2cb-0a0957f746ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 114MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Train Features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 75/75 [00:17<00:00,  4.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Validation Features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:04<00:00,  3.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SVM with GridSearch...\n",
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params: {'svm__C': 10, 'svm__gamma': 0.001}\n",
            "Best CV accuracy: 0.9883333333333333\n",
            "Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       320\n",
            "           1       0.99      0.99      0.99       280\n",
            "\n",
            "    accuracy                           0.99       600\n",
            "   macro avg       0.99      0.99      0.99       600\n",
            "weighted avg       0.99      0.99      0.99       600\n",
            "\n",
            "Validation Confusion Matrix:\n",
            " [[317   3]\n",
            " [  4 276]]\n",
            "\n",
            "Predicting on /pred folder...\n",
            "pred57.jpg → Predicted: no\n",
            "pred2.jpg → Predicted: yes\n",
            "pred26.jpg → Predicted: yes\n",
            "pred47.jpg → Predicted: no\n",
            "pred18.jpg → Predicted: no\n",
            "pred23.jpg → Predicted: yes\n",
            "pred49.jpg → Predicted: no\n",
            "pred12.jpg → Predicted: no\n",
            "pred10.jpg → Predicted: no\n",
            "pred9.jpg → Predicted: yes\n",
            "pred15.jpg → Predicted: yes\n",
            "pred41.jpg → Predicted: no\n",
            "pred48.jpg → Predicted: no\n",
            "pred37.jpg → Predicted: no\n",
            "pred17.jpg → Predicted: no\n",
            "pred8.jpg → Predicted: yes\n",
            "pred0.jpg → Predicted: no\n",
            "pred30.jpg → Predicted: yes\n",
            "pred19.jpg → Predicted: no\n",
            "pred53.jpg → Predicted: no\n",
            "pred1.jpg → Predicted: no\n",
            "pred4.jpg → Predicted: no\n",
            "pred20.jpg → Predicted: no\n",
            "pred35.jpg → Predicted: no\n",
            "pred44.jpg → Predicted: no\n",
            "pred33.jpg → Predicted: no\n",
            "pred59.jpg → Predicted: no\n",
            "pred24.jpg → Predicted: no\n",
            "pred25.jpg → Predicted: no\n",
            "pred6.jpg → Predicted: yes\n",
            "pred31.jpg → Predicted: no\n",
            "pred36.jpg → Predicted: no\n",
            "pred46.jpg → Predicted: no\n",
            "pred14.jpg → Predicted: yes\n",
            "pred55.jpg → Predicted: yes\n",
            "pred27.jpg → Predicted: no\n",
            "pred38.jpg → Predicted: no\n",
            "pred21.jpg → Predicted: no\n",
            "pred13.jpg → Predicted: yes\n",
            "pred3.jpg → Predicted: no\n",
            "pred40.jpg → Predicted: no\n",
            "pred39.jpg → Predicted: yes\n",
            "pred5.jpg → Predicted: yes\n",
            "pred7.jpg → Predicted: yes\n",
            "pred58.jpg → Predicted: no\n",
            "pred34.jpg → Predicted: yes\n",
            "pred56.jpg → Predicted: yes\n",
            "pred51.jpg → Predicted: no\n",
            "pred32.jpg → Predicted: no\n",
            "pred50.jpg → Predicted: no\n",
            "pred11.jpg → Predicted: yes\n",
            "pred22.jpg → Predicted: yes\n",
            "pred29.jpg → Predicted: no\n",
            "pred42.jpg → Predicted: no\n",
            "pred43.jpg → Predicted: no\n",
            "pred52.jpg → Predicted: no\n",
            "pred16.jpg → Predicted: yes\n",
            "pred45.jpg → Predicted: yes\n",
            "pred54.jpg → Predicted: no\n",
            "pred28.jpg → Predicted: yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#effcientB0+SVM(gridsearch)(no PCA and no optimizer)\n",
        "import torch\n",
        "from torchvision import models, transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Dataset\n",
        "dataset = datasets.ImageFolder(root=\"/content/sample_data/Train\", transform=transform)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_data, val_data = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=False)\n",
        "val_loader = DataLoader(val_data, batch_size=32)\n",
        "\n",
        "# Load EfficientNetB0\n",
        "model = models.efficientnet_b0(pretrained=True)\n",
        "model.classifier = torch.nn.Identity()  # output will be 1280-dim\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Feature extraction\n",
        "def extract_features(loader):\n",
        "    features = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, lbls in tqdm(loader):\n",
        "            images = images.to(device)\n",
        "            feats = model(images)  # shape: (batch_size, 1280)\n",
        "            features.append(feats.cpu().numpy())\n",
        "            labels.append(lbls.numpy())\n",
        "    features = np.vstack(features)\n",
        "    labels = np.concatenate(labels)\n",
        "    return features, labels\n",
        "\n",
        "# Extract features\n",
        "print(\"Extracting Train Features...\")\n",
        "train_features, train_labels = extract_features(train_loader)\n",
        "\n",
        "print(\"Extracting Validation Features...\")\n",
        "val_features, val_labels = extract_features(val_loader)\n",
        "\n",
        "# SVM pipeline (no PCA)\n",
        "svm = SVC(kernel='rbf', probability=True)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', svm)\n",
        "])\n",
        "\n",
        "# GridSearch: C and gamma\n",
        "param_grid = {\n",
        "    'svm__C': [10, 50, 100, 500],\n",
        "    'svm__gamma': [0.001, 0.005, 0.01]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=3)\n",
        "\n",
        "# Train SVM\n",
        "print(\"Training SVM...\")\n",
        "grid.fit(train_features, train_labels)\n",
        "\n",
        "# Results\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "print(\"Best CV accuracy:\", grid.best_score_)\n",
        "\n",
        "# Validation\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "val_preds = best_model.predict(val_features)\n",
        "\n",
        "print(\"Validation Classification Report:\\n\", classification_report(val_labels, val_preds))\n",
        "print(\"Validation Confusion Matrix:\\n\", confusion_matrix(val_labels, val_preds))\n",
        "\n",
        "# Predict on new images\n",
        "print(\"\\nPredicting on /pred folder...\")\n",
        "\n",
        "pred_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "pred_folder_path = \"/content/sample_data/pred\"\n",
        "class_names = dataset.classes\n",
        "\n",
        "for img_name in os.listdir(pred_folder_path):\n",
        "    img_path = os.path.join(pred_folder_path, img_name)\n",
        "    if os.path.isfile(img_path):\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            img_tensor = pred_transform(img).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                feat = model(img_tensor)\n",
        "            feat_np = feat.cpu().numpy()\n",
        "            pred_label = best_model.predict(feat_np)[0]\n",
        "            print(f\"{img_name} → Predicted: {class_names[pred_label]}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not process {img_name}: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZKIRfnYvdnY",
        "outputId": "5ca3acb3-cf40-404a-a09d-6ee5ff13df65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Train Features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 75/75 [00:18<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Validation Features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:04<00:00,  4.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SVM...\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params: {'svm__C': 10, 'svm__gamma': 0.001}\n",
            "Best CV accuracy: 0.9862500000000001\n",
            "Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99       290\n",
            "           1       0.98      1.00      0.99       310\n",
            "\n",
            "    accuracy                           0.99       600\n",
            "   macro avg       0.99      0.99      0.99       600\n",
            "weighted avg       0.99      0.99      0.99       600\n",
            "\n",
            "Validation Confusion Matrix:\n",
            " [[284   6]\n",
            " [  1 309]]\n",
            "\n",
            "Predicting on /pred folder...\n",
            "pred57.jpg → Predicted: no\n",
            "pred2.jpg → Predicted: yes\n",
            "pred26.jpg → Predicted: yes\n",
            "pred47.jpg → Predicted: no\n",
            "pred18.jpg → Predicted: no\n",
            "pred23.jpg → Predicted: yes\n",
            "pred49.jpg → Predicted: no\n",
            "pred12.jpg → Predicted: no\n",
            "pred10.jpg → Predicted: yes\n",
            "pred9.jpg → Predicted: yes\n",
            "pred15.jpg → Predicted: yes\n",
            "pred41.jpg → Predicted: no\n",
            "pred48.jpg → Predicted: no\n",
            "pred37.jpg → Predicted: no\n",
            "pred17.jpg → Predicted: no\n",
            "pred8.jpg → Predicted: yes\n",
            "pred0.jpg → Predicted: no\n",
            "pred30.jpg → Predicted: yes\n",
            "pred19.jpg → Predicted: no\n",
            "pred53.jpg → Predicted: no\n",
            "pred1.jpg → Predicted: no\n",
            "pred4.jpg → Predicted: no\n",
            "pred20.jpg → Predicted: no\n",
            "pred35.jpg → Predicted: no\n",
            "pred44.jpg → Predicted: no\n",
            "pred33.jpg → Predicted: no\n",
            "pred59.jpg → Predicted: no\n",
            "pred24.jpg → Predicted: no\n",
            "pred25.jpg → Predicted: no\n",
            "pred6.jpg → Predicted: yes\n",
            "pred31.jpg → Predicted: no\n",
            "pred36.jpg → Predicted: no\n",
            "pred46.jpg → Predicted: no\n",
            "pred14.jpg → Predicted: yes\n",
            "pred55.jpg → Predicted: yes\n",
            "pred27.jpg → Predicted: no\n",
            "pred38.jpg → Predicted: no\n",
            "pred21.jpg → Predicted: no\n",
            "pred13.jpg → Predicted: yes\n",
            "pred3.jpg → Predicted: no\n",
            "pred40.jpg → Predicted: no\n",
            "pred39.jpg → Predicted: yes\n",
            "pred5.jpg → Predicted: yes\n",
            "pred7.jpg → Predicted: yes\n",
            "pred58.jpg → Predicted: no\n",
            "pred34.jpg → Predicted: yes\n",
            "pred56.jpg → Predicted: yes\n",
            "pred51.jpg → Predicted: no\n",
            "pred32.jpg → Predicted: no\n",
            "pred50.jpg → Predicted: no\n",
            "pred11.jpg → Predicted: yes\n",
            "pred22.jpg → Predicted: yes\n",
            "pred29.jpg → Predicted: no\n",
            "pred42.jpg → Predicted: no\n",
            "pred43.jpg → Predicted: no\n",
            "pred52.jpg → Predicted: no\n",
            "pred16.jpg → Predicted: yes\n",
            "pred45.jpg → Predicted: yes\n",
            "pred54.jpg → Predicted: no\n",
            "pred28.jpg → Predicted: yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ResNet152 model fine-tuned with SGD + Momentum optimizer (no PCA)\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms, models\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 1. Data Transform (No augmentation for validation, only on train could be optionally added)\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# 2. Load dataset and split\n",
        "dataset_path = \"/content/sample_data/Train\"\n",
        "dataset = datasets.ImageFolder(root=dataset_path, transform=data_transform)\n",
        "num_classes = len(dataset.classes)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Training samples: {train_size}, Validation samples: {val_size}\")\n",
        "\n",
        "# 3. Load pretrained ResNet152\n",
        "model = models.resnet152(pretrained=True)\n",
        "\n",
        "# Freeze all layers first\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze layer4 and fc layers\n",
        "for param in model.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Replace fully connected layer\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(model.fc.in_features, 512),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(p=0.3),\n",
        "    nn.Linear(512, num_classes)\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# 4. Loss and optimizer: SGD with momentum\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "# 5. Training Loop\n",
        "num_epochs = 10\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    total_train = 0\n",
        "    correct_train = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (preds == labels).sum().item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    train_loss = running_loss / total_train\n",
        "    train_acc = correct_train / total_train\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    total_val = 0\n",
        "    correct_val = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (preds == labels).sum().item()\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss = val_loss / total_val\n",
        "    val_acc = correct_val / total_val\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "          f\"Train Loss: {train_loss:.4f} Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f} Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_resnet152_sgd_momentum.pth\")\n",
        "\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report on Validation Set:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=dataset.classes))\n",
        "\n",
        "print(\"\\nConfusion Matrix on Validation Set:\")\n",
        "print(confusion_matrix(all_labels, all_preds))\n",
        "\n",
        "# 6. Predict on new images folder\n",
        "pred_folder = \"/content/sample_data/pred\"\n",
        "pred_transform = data_transform\n",
        "model.load_state_dict(torch.load(\"best_resnet152_sgd_momentum.pth\"))\n",
        "model.eval()\n",
        "\n",
        "print(f\"\\nClass Names: {dataset.classes}\")\n",
        "print(\"\\nPredictions on /pred folder:\")\n",
        "\n",
        "for img_name in os.listdir(pred_folder):\n",
        "    img_path = os.path.join(pred_folder, img_name)\n",
        "    if os.path.isfile(img_path):\n",
        "        try:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "            img_tensor = pred_transform(img).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(img_tensor)\n",
        "                _, pred = torch.max(outputs, 1)\n",
        "                label = dataset.classes[pred.item()]\n",
        "            print(f\"{img_name} → Predicted: {label}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not process image {img_name}: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUC-9mswzLVn",
        "outputId": "bcf48f6a-a00a-4e45-875f-77b356306ec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 2\n",
            "Training samples: 2400, Validation samples: 600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Train Loss: 0.5261 Train Acc: 0.7717 | Val Loss: 0.3281 Val Acc: 0.8733\n",
            "Epoch [2/10] Train Loss: 0.2220 Train Acc: 0.9196 | Val Loss: 0.1542 Val Acc: 0.9383\n",
            "Epoch [3/10] Train Loss: 0.1019 Train Acc: 0.9650 | Val Loss: 0.0920 Val Acc: 0.9700\n",
            "Epoch [4/10] Train Loss: 0.0581 Train Acc: 0.9838 | Val Loss: 0.0757 Val Acc: 0.9750\n",
            "Epoch [5/10] Train Loss: 0.0278 Train Acc: 0.9950 | Val Loss: 0.0613 Val Acc: 0.9817\n",
            "Epoch [6/10] Train Loss: 0.0236 Train Acc: 0.9938 | Val Loss: 0.0656 Val Acc: 0.9783\n",
            "Epoch [7/10] Train Loss: 0.0211 Train Acc: 0.9958 | Val Loss: 0.0636 Val Acc: 0.9800\n",
            "Epoch [8/10] Train Loss: 0.0249 Train Acc: 0.9946 | Val Loss: 0.0720 Val Acc: 0.9783\n",
            "Epoch [9/10] Train Loss: 0.0199 Train Acc: 0.9954 | Val Loss: 0.0632 Val Acc: 0.9800\n",
            "Epoch [10/10] Train Loss: 0.0182 Train Acc: 0.9967 | Val Loss: 0.0725 Val Acc: 0.9783\n",
            "Best Validation Accuracy: 0.9817\n",
            "\n",
            "Classification Report on Validation Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          no       0.96      0.99      0.98       289\n",
            "         yes       0.99      0.96      0.98       311\n",
            "\n",
            "    accuracy                           0.98       600\n",
            "   macro avg       0.98      0.98      0.98       600\n",
            "weighted avg       0.98      0.98      0.98       600\n",
            "\n",
            "\n",
            "Confusion Matrix on Validation Set:\n",
            "[[287   2]\n",
            " [ 11 300]]\n",
            "\n",
            "Class Names: ['no', 'yes']\n",
            "\n",
            "Predictions on /pred folder:\n",
            "pred57.jpg → Predicted: no\n",
            "pred2.jpg → Predicted: yes\n",
            "pred26.jpg → Predicted: yes\n",
            "pred47.jpg → Predicted: no\n",
            "pred18.jpg → Predicted: no\n",
            "pred23.jpg → Predicted: yes\n",
            "pred49.jpg → Predicted: no\n",
            "pred12.jpg → Predicted: no\n",
            "pred10.jpg → Predicted: yes\n",
            "pred9.jpg → Predicted: yes\n",
            "pred15.jpg → Predicted: no\n",
            "pred41.jpg → Predicted: no\n",
            "pred48.jpg → Predicted: no\n",
            "pred37.jpg → Predicted: no\n",
            "pred17.jpg → Predicted: no\n",
            "pred8.jpg → Predicted: yes\n",
            "pred0.jpg → Predicted: no\n",
            "pred30.jpg → Predicted: no\n",
            "pred19.jpg → Predicted: no\n",
            "pred53.jpg → Predicted: no\n",
            "pred1.jpg → Predicted: no\n",
            "pred4.jpg → Predicted: no\n",
            "pred20.jpg → Predicted: no\n",
            "pred35.jpg → Predicted: no\n",
            "pred44.jpg → Predicted: no\n",
            "pred33.jpg → Predicted: no\n",
            "pred59.jpg → Predicted: no\n",
            "pred24.jpg → Predicted: no\n",
            "pred25.jpg → Predicted: no\n",
            "pred6.jpg → Predicted: yes\n",
            "pred31.jpg → Predicted: no\n",
            "pred36.jpg → Predicted: no\n",
            "pred46.jpg → Predicted: no\n",
            "pred14.jpg → Predicted: yes\n",
            "pred55.jpg → Predicted: yes\n",
            "pred27.jpg → Predicted: no\n",
            "pred38.jpg → Predicted: no\n",
            "pred21.jpg → Predicted: no\n",
            "pred13.jpg → Predicted: yes\n",
            "pred3.jpg → Predicted: no\n",
            "pred40.jpg → Predicted: no\n",
            "pred39.jpg → Predicted: yes\n",
            "pred5.jpg → Predicted: yes\n",
            "pred7.jpg → Predicted: yes\n",
            "pred58.jpg → Predicted: no\n",
            "pred34.jpg → Predicted: yes\n",
            "pred56.jpg → Predicted: yes\n",
            "pred51.jpg → Predicted: no\n",
            "pred32.jpg → Predicted: no\n",
            "pred50.jpg → Predicted: no\n",
            "pred11.jpg → Predicted: yes\n",
            "pred22.jpg → Predicted: yes\n",
            "pred29.jpg → Predicted: no\n",
            "pred42.jpg → Predicted: no\n",
            "pred43.jpg → Predicted: no\n",
            "pred52.jpg → Predicted: no\n",
            "pred16.jpg → Predicted: yes\n",
            "pred45.jpg → Predicted: yes\n",
            "pred54.jpg → Predicted: no\n",
            "pred28.jpg → Predicted: no\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ViT B16 + PCA(99%)\t~400\tSVM (RBF)\tGridSearch + no optimizer\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from transformers import ViTForImageClassification\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 1️⃣ Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5],\n",
        "                         [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# 2️⃣ Load dataset\n",
        "dataset = datasets.ImageFolder(root=\"/content/sample_data/Train\", transform=transform)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_data, val_data = random_split(dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
        "\n",
        "# 3️⃣ Load pretrained ViT-B16 for image classification\n",
        "vit_model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=len(dataset.classes))\n",
        "vit_model = vit_model.to(device)\n",
        "\n",
        "# 4️⃣ Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(vit_model.parameters(), lr=1e-4)\n",
        "\n",
        "# 5️⃣ Training Loop\n",
        "num_epochs = 10\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    vit_model.train()\n",
        "    running_loss = 0.0\n",
        "    total_train = 0\n",
        "    correct_train = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = vit_model(images).logits  # Get logits from the model\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (preds == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / total_train\n",
        "    train_acc = correct_train / total_train\n",
        "\n",
        "    # Validation\n",
        "    vit_model.eval()\n",
        "    val_loss = 0.0\n",
        "    total_val = 0\n",
        "    correct_val = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = vit_model(images).logits\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (preds == labels).sum().item()\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss = val_loss / total_val\n",
        "    val_acc = correct_val / total_val\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "          f\"Train Loss: {train_loss:.4f} Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f} Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(vit_model.state_dict(), \"best_vit_model.pth\")\n",
        "\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "# 6️⃣ Evaluate on validation set\n",
        "print(\"Validation Classification Report:\\n\", classification_report(all_labels, all_preds))\n",
        "print(\"Validation Confusion Matrix:\\n\", confusion_matrix(all_labels, all_preds))\n",
        "\n",
        "# 7️⃣ Predict on New Images\n",
        "pred_folder_path = \"/content/sample_data/pred\"\n",
        "pred_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5],\n",
        "                         [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "class_names = dataset.classes\n",
        "print(f\"\\nClass Names: {class_names}\")\n",
        "print(\"\\nPredictions on /pred folder:\")\n",
        "\n",
        "vit_model.eval()\n",
        "pred_features = []\n",
        "pred_img_names = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for img_name in os.listdir(pred_folder_path):\n",
        "        img_path = os.path.join(pred_folder_path, img_name)\n",
        "        if os.path.isfile(img_path):\n",
        "            try:\n",
        "                img = Image.open(img_path).convert('RGB')\n",
        "                img_tensor = pred_transform(img).unsqueeze(0).to(device)\n",
        "                output = vit_model(img_tensor).logits\n",
        "                pred = torch.argmax(output, dim=1).cpu().numpy()[0]\n",
        "                label = class_names[pred]\n",
        "                print(f\"{img_name} → Predicted: {label}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Could not process image {img_name}: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm1miWNt2beI",
        "outputId": "02b987b9-d27f-42f7-898b-01a28b13dace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Training Epoch 1: 100%|██████████| 75/75 [01:25<00:00,  1.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Train Loss: 0.1541 Train Acc: 0.9367 | Val Loss: 0.0315 Val Acc: 0.9917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 75/75 [01:23<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10] Train Loss: 0.0335 Train Acc: 0.9925 | Val Loss: 0.0119 Val Acc: 0.9983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 75/75 [01:23<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10] Train Loss: 0.0195 Train Acc: 0.9946 | Val Loss: 0.0124 Val Acc: 0.9967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 75/75 [01:23<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10] Train Loss: 0.0080 Train Acc: 0.9983 | Val Loss: 0.1122 Val Acc: 0.9683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 75/75 [01:23<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10] Train Loss: 0.0119 Train Acc: 0.9975 | Val Loss: 0.0120 Val Acc: 0.9983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 75/75 [01:24<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10] Train Loss: 0.0019 Train Acc: 1.0000 | Val Loss: 0.0120 Val Acc: 0.9983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 75/75 [01:23<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10] Train Loss: 0.0014 Train Acc: 1.0000 | Val Loss: 0.0120 Val Acc: 0.9983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 75/75 [01:23<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10] Train Loss: 0.0011 Train Acc: 1.0000 | Val Loss: 0.0121 Val Acc: 0.9983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 75/75 [01:23<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10] Train Loss: 0.0009 Train Acc: 1.0000 | Val Loss: 0.0122 Val Acc: 0.9983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 75/75 [01:23<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10] Train Loss: 0.0008 Train Acc: 1.0000 | Val Loss: 0.0124 Val Acc: 0.9983\n",
            "Best Validation Accuracy: 0.9983\n",
            "Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       319\n",
            "           1       1.00      1.00      1.00       281\n",
            "\n",
            "    accuracy                           1.00       600\n",
            "   macro avg       1.00      1.00      1.00       600\n",
            "weighted avg       1.00      1.00      1.00       600\n",
            "\n",
            "Validation Confusion Matrix:\n",
            " [[319   0]\n",
            " [  1 280]]\n",
            "\n",
            "Class Names: ['no', 'yes']\n",
            "\n",
            "Predictions on /pred folder:\n",
            "pred57.jpg → Predicted: no\n",
            "pred2.jpg → Predicted: yes\n",
            "pred26.jpg → Predicted: no\n",
            "pred47.jpg → Predicted: no\n",
            "pred18.jpg → Predicted: no\n",
            "pred23.jpg → Predicted: yes\n",
            "pred49.jpg → Predicted: no\n",
            "pred12.jpg → Predicted: no\n",
            "pred10.jpg → Predicted: yes\n",
            "pred9.jpg → Predicted: yes\n",
            "pred15.jpg → Predicted: no\n",
            "pred41.jpg → Predicted: no\n",
            "pred48.jpg → Predicted: no\n",
            "pred37.jpg → Predicted: no\n",
            "pred17.jpg → Predicted: no\n",
            "pred8.jpg → Predicted: yes\n",
            "pred0.jpg → Predicted: no\n",
            "pred30.jpg → Predicted: yes\n",
            "pred19.jpg → Predicted: no\n",
            "pred53.jpg → Predicted: no\n",
            "pred1.jpg → Predicted: no\n",
            "pred4.jpg → Predicted: no\n",
            "pred20.jpg → Predicted: no\n",
            "pred35.jpg → Predicted: no\n",
            "pred44.jpg → Predicted: no\n",
            "pred33.jpg → Predicted: no\n",
            "pred59.jpg → Predicted: no\n",
            "pred24.jpg → Predicted: no\n",
            "pred25.jpg → Predicted: no\n",
            "pred6.jpg → Predicted: yes\n",
            "pred31.jpg → Predicted: no\n",
            "pred36.jpg → Predicted: no\n",
            "pred46.jpg → Predicted: no\n",
            "pred14.jpg → Predicted: yes\n",
            "pred55.jpg → Predicted: no\n",
            "pred27.jpg → Predicted: no\n",
            "pred38.jpg → Predicted: no\n",
            "pred21.jpg → Predicted: no\n",
            "pred13.jpg → Predicted: yes\n",
            "pred3.jpg → Predicted: no\n",
            "pred40.jpg → Predicted: no\n",
            "pred39.jpg → Predicted: yes\n",
            "pred5.jpg → Predicted: yes\n",
            "pred7.jpg → Predicted: yes\n",
            "pred58.jpg → Predicted: no\n",
            "pred34.jpg → Predicted: yes\n",
            "pred56.jpg → Predicted: yes\n",
            "pred51.jpg → Predicted: no\n",
            "pred32.jpg → Predicted: no\n",
            "pred50.jpg → Predicted: no\n",
            "pred11.jpg → Predicted: yes\n",
            "pred22.jpg → Predicted: no\n",
            "pred29.jpg → Predicted: no\n",
            "pred42.jpg → Predicted: no\n",
            "pred43.jpg → Predicted: no\n",
            "pred52.jpg → Predicted: no\n",
            "pred16.jpg → Predicted: yes\n",
            "pred45.jpg → Predicted: yes\n",
            "pred54.jpg → Predicted: no\n",
            "pred28.jpg → Predicted: yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Import libs\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# 2️⃣ Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 3️⃣ Transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # ViT-B16 expects 224x224\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # ViT usually trained with this norm\n",
        "])\n",
        "\n",
        "# 4️⃣ Load dataset\n",
        "dataset = datasets.ImageFolder(root=\"/content/sample_data/Train\", transform=transform)\n",
        "num_classes = len(dataset.classes)\n",
        "print(f\"Classes: {dataset.classes}\")\n",
        "\n",
        "# Split\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_data, val_data = random_split(dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=16)\n",
        "\n",
        "# 5️⃣ Load pretrained ViT-B16 model\n",
        "from transformers import ViTForImageClassification\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    \"google/vit-base-patch16-224-in21k\",  # ViT-B16 pretrained on ImageNet-21k\n",
        "    num_labels=num_classes\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# 6️⃣ Define optimizer (AdamW) and loss\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# 7️⃣ Training loop with accuracy\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images).logits\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Accuracy\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f} - Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 8️⃣ Validation\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images).logits\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "# Final average accuracy\n",
        "final_val_accuracy = 100 * correct / total\n",
        "print(f\"\\n✅ Final Validation Accuracy: {final_val_accuracy:.2f}%\")\n",
        "\n",
        "# 9️⃣ Metrics\n",
        "print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds, target_names=dataset.classes))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(all_labels, all_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664,
          "referenced_widgets": [
            "40c206ecfd3544599b817363f90bb08d",
            "e81c9d22596f4c40ad384ba414211b37",
            "847e772b15d5450fbfdd0b84ee84b72f",
            "44d93259c03c481abf6ba70f8b309197",
            "4933b3b84a2d404cac185e78785b6b8a",
            "383957d8537747c4bc420adec6cc859b",
            "af50bba9752246fc9b18cc0ea9de8ace",
            "0ad318024717499a89ad765cdae29c26",
            "305b88964b7a4ed696398beb442a4ea5",
            "da5f86c75888426d9bda68866c1d3307",
            "15af139400234d6c8837920975fc2cef",
            "e21aeaa2e1614c7b9b63f9de00f8d278",
            "9b3cdf621f404d8a939e2a2c24e655a1",
            "d7c50e061e46498886fc14631f91be43",
            "4e3d13a666534a9aa174697a9ea255cc",
            "2c50024d6de34082b75ae622303f6468",
            "31b02f2114c7447894d2bfe96ab9e47c",
            "473e9d2acd5e4533a47189b79a14eea3",
            "f27b74e0003c4c51b541e88664a20c5f",
            "e4afb9c0514748c3b1ff85456c75e2e3",
            "52f7468f6d504815ab0e5f007e37e6e2",
            "c25c7e0f87c14ab7b2c59521a8335d4a"
          ]
        },
        "id": "rLtGCa0CByI3",
        "outputId": "4a1212d4-18ce-4713-c9cc-2bf962a23349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Classes: ['no', 'yes']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40c206ecfd3544599b817363f90bb08d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e21aeaa2e1614c7b9b63f9de00f8d278"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5] - Loss: 0.1426 - Accuracy: 95.75%\n",
            "Epoch [2/5] - Loss: 0.0263 - Accuracy: 99.42%\n",
            "Epoch [3/5] - Loss: 0.0250 - Accuracy: 99.33%\n",
            "Epoch [4/5] - Loss: 0.0114 - Accuracy: 99.71%\n",
            "Epoch [5/5] - Loss: 0.0104 - Accuracy: 99.83%\n",
            "\n",
            "✅ Final Validation Accuracy: 99.67%\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          no       1.00      0.99      1.00       290\n",
            "         yes       0.99      1.00      1.00       310\n",
            "\n",
            "    accuracy                           1.00       600\n",
            "   macro avg       1.00      1.00      1.00       600\n",
            "weighted avg       1.00      1.00      1.00       600\n",
            "\n",
            "Confusion Matrix:\n",
            " [[288   2]\n",
            " [  0 310]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()  # Upload kaggle.json\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Use the Kaggle API to download (replace this string with your dataset’s URL suffix)\n",
        "!kaggle datasets download ahmedhamada0/brain-tumor-detection -p /content/ --unzip\n"
      ],
      "metadata": {
        "id": "OdfPAhm-_Wlm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "outputId": "c15469b5-89d2-467a-efeb-45c9ae874579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.7.14)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1031c5d4-d584-424c-b175-bffee1c2588a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1031c5d4-d584-424c-b175-bffee1c2588a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Dataset URL: https://www.kaggle.com/datasets/ahmedhamada0/brain-tumor-detection\n",
            "License(s): copyright-authors\n",
            "Downloading brain-tumor-detection.zip to /content\n",
            "  0% 0.00/84.0M [00:00<?, ?B/s]\n",
            "100% 84.0M/84.0M [00:00<00:00, 1.35GB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "poLRqqQc0YtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dVLE8ODu0YxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from PIL import Image\n",
        "import timm  # PyTorch Image Models library\n",
        "\n",
        "# --- 1. Configuration and Setup ---\n",
        "print(\"--- 1. Configuring Environment and Parameters ---\")\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Parameters\n",
        "DATA_DIR = \"/content/sample_data/Train\"  # Use the root folder containing 'yes' and 'no' subdirectories\n",
        "PRED_DIR = \"/content/sample_data/pred\"\n",
        "N_SPLITS = 10  # Number of folds for cross-validation\n",
        "BATCH_SIZE = 32\n",
        "RANDOM_STATE = 42\n",
        "MODEL_NAME = 'convnext_tiny'  # Changed to ConvNeXt model\n",
        "\n",
        "# --- 2. Advanced Data Augmentation and Transforms ---\n",
        "print(\"--- 2. Defining Advanced Data Augmentations ---\")\n",
        "# Define more robust transformations for training to simulate real-world variance\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ElasticTransform(alpha=50.0, sigma=5.0),  # Simulate tissue deformation\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Simulate scanner differences\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Use ImageNet stats as a good starting point\n",
        "])\n",
        "\n",
        "# For validation/testing, we only need to resize and normalize\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# --- 3. Dataset and Model Loading ---\n",
        "print(\"--- 3. Loading Dataset and ConvNeXt Model ---\")\n",
        "# Load the full dataset. Splitting will be handled by StratifiedKFold.\n",
        "full_dataset = datasets.ImageFolder(root=DATA_DIR)\n",
        "class_names = full_dataset.classes\n",
        "print(f\"Classes found: {class_names}\")\n",
        "\n",
        "# Load the ConvNeXt model from timm\n",
        "# num_classes=0 removes the final classifier head, making it a pure feature extractor.\n",
        "model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=0).to(device)\n",
        "model.eval()  # Set model to evaluation mode as we are only extracting features.\n",
        "\n",
        "# --- 4. Helper Function for Feature Extraction ---\n",
        "def extract_features(dataloader, model, device):\n",
        "    \"\"\"\n",
        "    Extracts deep features from a dataset using the given model.\n",
        "    \"\"\"\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Extracting Features\"):\n",
        "            images = images.to(device)\n",
        "            features = model(images)  # This gives a (batch_size, feature_dim) tensor\n",
        "            all_features.append(features.cpu().numpy())\n",
        "            all_labels.append(labels.numpy())\n",
        "\n",
        "    return np.concatenate(all_features), np.concatenate(all_labels)\n",
        "\n",
        "# --- 5. Stratified K-Fold Cross-Validation ---\n",
        "print(\"\\n--- 5. Starting Stratified 10-Fold Cross-Validation ---\")\n",
        "\n",
        "# Prepare for K-Fold\n",
        "targets = np.array(full_dataset.targets)\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "# Lists to store metrics from each fold\n",
        "fold_accuracies = []\n",
        "fold_sensitivities = []  # Recall\n",
        "fold_specificities = []\n",
        "\n",
        "# Main loop for cross-validation\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(targets)), targets)):\n",
        "    print(f\"\\n===== FOLD {fold + 1}/{N_SPLITS} =====\")\n",
        "\n",
        "    # Create train and validation subsets for the current fold\n",
        "    # Apply the correct transform to each subset\n",
        "    train_subset = Subset(full_dataset, train_idx)\n",
        "    train_subset.dataset.transform = train_transform  # Apply training transforms\n",
        "    val_subset = Subset(full_dataset, val_idx)\n",
        "    val_subset.dataset.transform = val_transform  # Apply validation transforms\n",
        "\n",
        "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # 1. Feature Extraction\n",
        "    X_train, y_train = extract_features(train_loader, model, device)\n",
        "    X_val, y_val = extract_features(val_loader, model, device)\n",
        "\n",
        "    # 2. Classifier Training (PCA + SVM with GridSearch)\n",
        "    # Define the pipeline\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('pca', PCA(n_components=0.99)),  # Retain 99% of variance\n",
        "        ('svm', SVC(kernel='rbf', random_state=RANDOM_STATE))\n",
        "    ])\n",
        "\n",
        "    # Define the parameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'svm__C': [10, 50, 100],\n",
        "        'svm__gamma': [0.01, 0.005, 0.001, 'scale']\n",
        "    }\n",
        "\n",
        "    # GridSearch finds the best SVM parameters using cross-validation within the training data\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1, verbose=1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Best SVM params for fold {fold + 1}: {grid_search.best_params_}\")\n",
        "\n",
        "    # 3. Evaluation on the validation set of the current fold\n",
        "    y_pred = grid_search.predict(X_val)\n",
        "\n",
        "    # Calculate metrics\n",
        "    report = classification_report(y_val, y_pred, target_names=class_names, output_dict=True)\n",
        "    cm = confusion_matrix(y_val, y_pred)\n",
        "\n",
        "    accuracy = report['accuracy']\n",
        "    sensitivity = report[class_names[1]]['recall']  # Recall for the 'yes' or positive class\n",
        "\n",
        "    # Specificity = TN / (TN + FP)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    specificity = tn / (tn + fp)\n",
        "\n",
        "    # Store metrics\n",
        "    fold_accuracies.append(accuracy)\n",
        "    fold_sensitivities.append(sensitivity)\n",
        "    fold_specificities.append(specificity)\n",
        "\n",
        "    print(f\"Fold {fold + 1} Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Fold {fold + 1} Sensitivity (Recall): {sensitivity:.4f}\")\n",
        "    print(f\"Fold {fold + 1} Specificity: {specificity:.4f}\")\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# --- 6. Final Performance Report ---\n",
        "print(\"\\n--- 6. Final Cross-Validation Performance Report ---\")\n",
        "print(f\"Average Accuracy:    {np.mean(fold_accuracies):.4f} ± {np.std(fold_accuracies):.4f}\")\n",
        "print(f\"Average Sensitivity: {np.mean(fold_sensitivities):.4f} ± {np.std(fold_sensitivities):.4f}\")\n",
        "print(f\"Average Specificity: {np.mean(fold_specificities):.4f} ± {np.std(fold_specificities):.4f}\")\n",
        "\n",
        "# --- 7. Explainable AI (XAI) - Visualizing Model Attention ---\n",
        "# This section demonstrates how to visualize what the model is looking at.\n",
        "# We will use the 'captum' library.\n",
        "\n",
        "try:\n",
        "    from captum.attr import LayerAttribution, visualization as viz\n",
        "    from captum.attr._utils.attribution import LayerAttribution\n",
        "\n",
        "    print(\"\\n--- 7. Generating XAI Attention Map for a Sample Image ---\")\n",
        "\n",
        "    # This wrapper is needed for captum to work with timm models\n",
        "    class ModelWrapper(torch.nn.Module):\n",
        "        def __init__(self, model):\n",
        "            super().__init__()\n",
        "            self.model = model\n",
        "\n",
        "        def forward(self, x):\n",
        "            return self.model(x)\n",
        "\n",
        "    # You need to find a layer to attribute. For ConvNeXt, an attention block in the last stage is a good choice.\n",
        "    # This requires inspecting the model architecture.\n",
        "    # For convnext_tiny, this is a valid layer.\n",
        "    attribution_layer = model.blocks[-1].norm  # Adjusted for ConvNeXt\n",
        "\n",
        "    model_wrapper = ModelWrapper(model).to(device)\n",
        "    layer_attr = LayerAttribution(model_wrapper, attribution_layer)\n",
        "\n",
        "    # Pick a sample image to explain (e.g., the first image in the prediction folder)\n",
        "    if os.path.exists(PRED_DIR) and len(os.listdir(PRED_DIR)) > 0:\n",
        "        sample_img_name = os.listdir(PRED_DIR)[0]\n",
        "        sample_img_path = os.path.join(PRED_DIR, sample_img_name)\n",
        "\n",
        "        img = Image.open(sample_img_path).convert('RGB')\n",
        "        input_tensor = val_transform(img).unsqueeze(0).to(device)\n",
        "        input_tensor.requires_grad = True  # Required for attribution\n",
        "\n",
        "        # Get the model's prediction\n",
        "        final_pipeline = grid_search.best_estimator_  # Use the last trained pipeline as an example\n",
        "        with torch.no_grad():\n",
        "            img_features = model(input_tensor)\n",
        "\n",
        "        prediction = final_pipeline.predict(img_features.cpu().numpy())[0]\n",
        "\n",
        "        # Calculate attribution. We attribute to the predicted class.\n",
        "        attribution_map = layer_attr.attribute(input_tensor, target=torch.tensor(prediction).to(device), relu_attributions=True)\n",
        "        # Average attribution across channels for visualization\n",
        "        attribution_map_viz = attribution_map.squeeze().cpu().detach().numpy().mean(axis=0)\n",
        "\n",
        "        # Visualize the result\n",
        "        viz.visualize_image_attr(\n",
        "            attribution_map_viz[np.newaxis, :, :],  # Add channel dimension back\n",
        "            np.transpose(input_tensor.squeeze().cpu().detach().numpy(), (1, 2, 0)),  # Original image\n",
        "            method='blended_heat_map',\n",
        "            sign='all',\n",
        "            show_colorbar=True,\n",
        "            title=f\"Attention Map for '{sample_img_name}'\\nPredicted: {class_names[prediction]}\"\n",
        "        )\n",
        "    else:\n",
        "        print(\"Prediction directory not found or empty. Skipping XAI visualization.\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"\\nCaptum not installed. Skipping XAI visualization. To install: pip install captum\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during XAI visualization: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wsvtg9SAgFau",
        "outputId": "abc8ae26-cd5d-47a2-ab76-2ac4991c7daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Configuring Environment and Parameters ---\n",
            "Using device: cuda\n",
            "--- 2. Defining Advanced Data Augmentations ---\n",
            "--- 3. Loading Dataset and ConvNeXt Model ---\n",
            "Classes found: ['no', 'yes']\n",
            "\n",
            "--- 5. Starting Stratified 10-Fold Cross-Validation ---\n",
            "\n",
            "===== FOLD 1/10 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features: 100%|██████████| 85/85 [00:39<00:00,  2.15it/s]\n",
            "Extracting Features: 100%|██████████| 10/10 [00:04<00:00,  2.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "Best SVM params for fold 1: {'svm__C': 10, 'svm__gamma': 'scale'}\n",
            "Fold 1 Accuracy: 0.9933\n",
            "Fold 1 Sensitivity (Recall): 0.9933\n",
            "Fold 1 Specificity: 0.9933\n",
            "Confusion Matrix:\n",
            " [[149   1]\n",
            " [  1 149]]\n",
            "\n",
            "===== FOLD 2/10 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features: 100%|██████████| 85/85 [00:19<00:00,  4.38it/s]\n",
            "Extracting Features: 100%|██████████| 10/10 [00:02<00:00,  4.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "Best SVM params for fold 2: {'svm__C': 10, 'svm__gamma': 0.001}\n",
            "Fold 2 Accuracy: 0.9867\n",
            "Fold 2 Sensitivity (Recall): 0.9933\n",
            "Fold 2 Specificity: 0.9800\n",
            "Confusion Matrix:\n",
            " [[147   3]\n",
            " [  1 149]]\n",
            "\n",
            "===== FOLD 3/10 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features: 100%|██████████| 85/85 [00:21<00:00,  3.93it/s]\n",
            "Extracting Features: 100%|██████████| 10/10 [00:02<00:00,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "Best SVM params for fold 3: {'svm__C': 10, 'svm__gamma': 0.001}\n",
            "Fold 3 Accuracy: 0.9933\n",
            "Fold 3 Sensitivity (Recall): 0.9933\n",
            "Fold 3 Specificity: 0.9933\n",
            "Confusion Matrix:\n",
            " [[149   1]\n",
            " [  1 149]]\n",
            "\n",
            "===== FOLD 4/10 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features: 100%|██████████| 85/85 [00:19<00:00,  4.31it/s]\n",
            "Extracting Features: 100%|██████████| 10/10 [00:02<00:00,  4.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "Best SVM params for fold 4: {'svm__C': 10, 'svm__gamma': 0.001}\n",
            "Fold 4 Accuracy: 0.9933\n",
            "Fold 4 Sensitivity (Recall): 0.9933\n",
            "Fold 4 Specificity: 0.9933\n",
            "Confusion Matrix:\n",
            " [[149   1]\n",
            " [  1 149]]\n",
            "\n",
            "===== FOLD 5/10 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features: 100%|██████████| 85/85 [00:19<00:00,  4.41it/s]\n",
            "Extracting Features: 100%|██████████| 10/10 [00:02<00:00,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "Best SVM params for fold 5: {'svm__C': 10, 'svm__gamma': 'scale'}\n",
            "Fold 5 Accuracy: 0.9867\n",
            "Fold 5 Sensitivity (Recall): 0.9867\n",
            "Fold 5 Specificity: 0.9867\n",
            "Confusion Matrix:\n",
            " [[148   2]\n",
            " [  2 148]]\n",
            "\n",
            "===== FOLD 6/10 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features: 100%|██████████| 85/85 [00:19<00:00,  4.35it/s]\n",
            "Extracting Features: 100%|██████████| 10/10 [00:02<00:00,  4.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "Best SVM params for fold 6: {'svm__C': 10, 'svm__gamma': 0.001}\n",
            "Fold 6 Accuracy: 0.9900\n",
            "Fold 6 Sensitivity (Recall): 0.9933\n",
            "Fold 6 Specificity: 0.9867\n",
            "Confusion Matrix:\n",
            " [[148   2]\n",
            " [  1 149]]\n",
            "\n",
            "===== FOLD 7/10 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features: 100%|██████████| 85/85 [00:19<00:00,  4.43it/s]\n",
            "Extracting Features: 100%|██████████| 10/10 [00:02<00:00,  4.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "Best SVM params for fold 7: {'svm__C': 10, 'svm__gamma': 'scale'}\n",
            "Fold 7 Accuracy: 0.9933\n",
            "Fold 7 Sensitivity (Recall): 1.0000\n",
            "Fold 7 Specificity: 0.9867\n",
            "Confusion Matrix:\n",
            " [[148   2]\n",
            " [  0 150]]\n",
            "\n",
            "===== FOLD 8/10 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features: 100%|██████████| 85/85 [00:19<00:00,  4.36it/s]\n",
            "Extracting Features: 100%|██████████| 10/10 [00:02<00:00,  4.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "Best SVM params for fold 8: {'svm__C': 10, 'svm__gamma': 0.001}\n",
            "Fold 8 Accuracy: 0.9967\n",
            "Fold 8 Sensitivity (Recall): 0.9933\n",
            "Fold 8 Specificity: 1.0000\n",
            "Confusion Matrix:\n",
            " [[150   0]\n",
            " [  1 149]]\n",
            "\n",
            "===== FOLD 9/10 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features: 100%|██████████| 85/85 [00:19<00:00,  4.35it/s]\n",
            "Extracting Features: 100%|██████████| 10/10 [00:02<00:00,  4.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "Best SVM params for fold 9: {'svm__C': 10, 'svm__gamma': 0.001}\n",
            "Fold 9 Accuracy: 0.9933\n",
            "Fold 9 Sensitivity (Recall): 1.0000\n",
            "Fold 9 Specificity: 0.9867\n",
            "Confusion Matrix:\n",
            " [[148   2]\n",
            " [  0 150]]\n",
            "\n",
            "===== FOLD 10/10 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features: 100%|██████████| 85/85 [00:19<00:00,  4.45it/s]\n",
            "Extracting Features: 100%|██████████| 10/10 [00:02<00:00,  4.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "Best SVM params for fold 10: {'svm__C': 10, 'svm__gamma': 'scale'}\n",
            "Fold 10 Accuracy: 0.9867\n",
            "Fold 10 Sensitivity (Recall): 0.9800\n",
            "Fold 10 Specificity: 0.9933\n",
            "Confusion Matrix:\n",
            " [[149   1]\n",
            " [  3 147]]\n",
            "\n",
            "--- 6. Final Cross-Validation Performance Report ---\n",
            "Average Accuracy:    0.9913 ± 0.0034\n",
            "Average Sensitivity: 0.9927 ± 0.0055\n",
            "Average Specificity: 0.9900 ± 0.0054\n",
            "\n",
            "--- 7. Generating XAI Attention Map for a Sample Image ---\n",
            "\n",
            "An error occurred during XAI visualization: 'ConvNeXt' object has no attribute 'blocks'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision scikit-learn tqdm Pillow timm captum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRuC6bQqjDXW",
        "outputId": "a213946c-8a49-4083-eba9-8a283b166c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.18)\n",
            "Requirement already satisfied: captum in /usr/local/lib/python3.11/dist-packages (0.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.33.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from captum) (3.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from captum) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from PIL import Image\n",
        "import timm\n",
        "from captum.attr import LayerGradCam, visualization as viz\n",
        "import warnings\n",
        "\n",
        "# Suppress minor warnings for cleaner output\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "# --- 1. Configuration: The Dual-Architecture Setup ---\n",
        "print(\"--- 1. Configuring Environment for Dual-Architecture Framework ---\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Parameters\n",
        "DATA_DIR = \"/content/sample_data/Train\"\n",
        "PRED_DIR = \"/content/sample_data/pred\"\n",
        "N_SPLITS = 10\n",
        "BATCH_SIZE = 32\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# Define our two diverse model architectures\n",
        "MODEL_1_NAME = 'swin_tiny_patch4_window7_224' # Transformer Family\n",
        "MODEL_2_NAME = 'convnext_tiny.in12k_ft_in1k' # Modern CNN Family\n",
        "\n",
        "# --- 2. Advanced Data Augmentation ---\n",
        "print(\"--- 2. Defining Advanced Data Augmentations ---\")\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ElasticTransform(alpha=50.0, sigma=5.0),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# --- 3. Model Loading ---\n",
        "print(f\"--- 3. Loading Models: '{MODEL_1_NAME}' and '{MODEL_2_NAME}' ---\")\n",
        "full_dataset = datasets.ImageFolder(root=DATA_DIR)\n",
        "class_names = full_dataset.classes\n",
        "\n",
        "# Load Model 1 (Swin Transformer)\n",
        "model_1 = timm.create_model(MODEL_1_NAME, pretrained=True, num_classes=0).to(device)\n",
        "model_1.eval()\n",
        "\n",
        "# Load Model 2 (ConvNeXt)\n",
        "model_2 = timm.create_model(MODEL_2_NAME, pretrained=True, num_classes=0).to(device)\n",
        "model_2.eval()\n",
        "\n",
        "# --- 4. Dual Feature Extraction ---\n",
        "def extract_dual_features(dataloader, m1, m2, device):\n",
        "    features1, features2, labels_all = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Extracting Dual Features\"):\n",
        "            images = images.to(device)\n",
        "            f1 = m1(images)\n",
        "            f2 = m2(images)\n",
        "            features1.append(f1.cpu().numpy())\n",
        "            features2.append(f2.cpu().numpy())\n",
        "            labels_all.append(labels.numpy())\n",
        "    return np.concatenate(features1), np.concatenate(features2), np.concatenate(labels_all)\n",
        "\n",
        "# --- 5. Stratified K-Fold with the Dual-Architecture Framework ---\n",
        "print(\"\\n--- 5. Starting Stratified 10-Fold Cross-Validation ---\")\n",
        "targets = np.array(full_dataset.targets)\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "# Store metrics\n",
        "fold_accuracies, fold_sensitivities, fold_specificities, fold_disagreement_rates = [], [], [], []\n",
        "\n",
        "# --- Helper to create a pipeline ---\n",
        "def create_svm_pipeline():\n",
        "    return Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('pca', PCA(n_components=0.99)),\n",
        "        ('svm', SVC(kernel='rbf', probability=True, random_state=RANDOM_STATE)) # probability=True is useful\n",
        "    ])\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(targets)), targets)):\n",
        "    print(f\"\\n===== FOLD {fold+1}/{N_SPLITS} =====\")\n",
        "    # Create DataLoaders for the current fold\n",
        "    train_subset = Subset(full_dataset, train_idx); train_subset.dataset.transform = train_transform\n",
        "    val_subset = Subset(full_dataset, val_idx); val_subset.dataset.transform = val_transform\n",
        "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # 1. Extract features from BOTH models\n",
        "    X_train1, X_train2, y_train = extract_dual_features(train_loader, model_1, model_2, device)\n",
        "    X_val1, X_val2, y_val = extract_dual_features(val_loader, model_1, model_2, device)\n",
        "\n",
        "    # 2. Train TWO separate SVM pipelines\n",
        "    print(\"Training SVM for Swin Transformer...\")\n",
        "    pipeline1 = create_svm_pipeline().fit(X_train1, y_train)\n",
        "    print(\"Training SVM for ConvNeXt...\")\n",
        "    pipeline2 = create_svm_pipeline().fit(X_train2, y_train)\n",
        "\n",
        "    # 3. Get predictions from BOTH pipelines\n",
        "    y_pred1 = pipeline1.predict(X_val1)\n",
        "    y_pred2 = pipeline2.predict(X_val2)\n",
        "\n",
        "    # 4. Implement the Novelty: Confidence-Aware Ensemble\n",
        "    final_preds = []\n",
        "    disagreements = 0\n",
        "    for i in range(len(y_val)):\n",
        "        # If models agree, that's the prediction\n",
        "        if y_pred1[i] == y_pred2[i]:\n",
        "            final_preds.append(y_pred1[i])\n",
        "        # If they disagree, flag it and use a tie-breaker (e.g., majority vote, here we default to model 1)\n",
        "        # In a real system, this would be \"UNCERTAIN\"\n",
        "        else:\n",
        "            disagreements += 1\n",
        "            final_preds.append(y_pred1[i]) # Simple tie-breaker\n",
        "\n",
        "    final_preds = np.array(final_preds)\n",
        "\n",
        "    # 5. Evaluate the final, high-confidence predictions\n",
        "    report = classification_report(y_val, final_preds, target_names=class_names, output_dict=True)\n",
        "    cm = confusion_matrix(y_val, final_preds)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    fold_accuracies.append(report['accuracy'])\n",
        "    fold_sensitivities.append(tp / (tp + fn))\n",
        "    fold_specificities.append(tn / (tn + fp))\n",
        "    fold_disagreement_rates.append(disagreements / len(y_val))\n",
        "\n",
        "    print(f\"Fold {fold+1} Final Accuracy: {report['accuracy']:.4f}\")\n",
        "    print(f\"Fold {fold+1} Disagreement Rate: {disagreements / len(y_val):.4f} ({disagreements} cases flagged)\")\n",
        "\n",
        "# --- 6. Final Performance Report ---\n",
        "print(\"\\n--- 6. Final Dual-Architecture Framework Performance Report ---\")\n",
        "print(f\"Average Accuracy:      {np.mean(fold_accuracies):.4f} ± {np.std(fold_accuracies):.4f}\")\n",
        "print(f\"Average Sensitivity:   {np.mean(fold_sensitivities):.4f} ± {np.std(fold_sensitivities):.4f}\")\n",
        "print(f\"Average Specificity:   {np.mean(fold_specificities):.4f} ± {np.std(fold_specificities):.4f}\")\n",
        "print(f\"Average Disagreement Rate: {np.mean(fold_disagreement_rates):.2%} (Flagged for human review)\")\n",
        "\n",
        "# --- 7. The Novelty Payoff: Comparative Explainability ---\n",
        "print(\"\\n--- 7. Generating Comparative XAI for a Disagreement Case ---\")\n",
        "\n",
        "# Find a disagreement case from the last fold for demonstration\n",
        "disagreement_indices = np.where(y_pred1 != y_pred2)[0]\n",
        "\n",
        "if len(disagreement_indices) > 0:\n",
        "    # Get the global index of the first disagreement case\n",
        "    disagree_idx_local = disagreement_indices[0]\n",
        "    disagree_idx_global = val_idx[disagree_idx_local]\n",
        "\n",
        "    # Get the image and its true label\n",
        "    image_tensor, true_label_idx = val_subset[disagree_idx_local]\n",
        "    image_tensor_unsqueezed = image_tensor.unsqueeze(0).to(device)\n",
        "    true_label = class_names[true_label_idx]\n",
        "\n",
        "    # Get predictions from both models\n",
        "    pred1_label = class_names[y_pred1[disagree_idx_local]]\n",
        "    pred2_label = class_names[y_pred2[disagree_idx_local]]\n",
        "\n",
        "    print(f\"Analyzing a disagreement case. True Label: '{true_label}'\")\n",
        "    print(f\"Swin Transformer Predicted: '{pred1_label}'\")\n",
        "    print(f\"ConvNeXt Predicted: '{pred2_label}'\")\n",
        "\n",
        "    # Prepare models and layers for Grad-CAM\n",
        "    # For Swin-T\n",
        "    model1_wrapper = type('ModelWrapper', (torch.nn.Module,), {'forward': lambda self, x: self.model(x), '__init__': lambda self, model: setattr(self, 'model', model)})(model_1)\n",
        "    layer1_gc = LayerGradCam(model1_wrapper, model_1.layers[-1].blocks[-1].norm1)\n",
        "\n",
        "    # For ConvNeXt\n",
        "    model2_wrapper = type('ModelWrapper', (torch.nn.Module,), {'forward': lambda self, x: self.model(x), '__init__': lambda self, model: setattr(self, 'model', model)})(model_2)\n",
        "    layer2_gc = LayerGradCam(model2_wrapper, model_2.stages[-1].blocks[-1].norm)\n",
        "\n",
        "    # Generate attributions for both\n",
        "    attr1 = layer1_gc.attribute(image_tensor_unsqueezed, target=y_pred1[disagree_idx_local])\n",
        "    attr2 = layer2_gc.attribute(image_tensor_unsqueezed, target=y_pred2[disagree_idx_local])\n",
        "\n",
        "    # Visualize side-by-side\n",
        "    original_image_np = np.transpose(image_tensor.cpu().numpy(), (1,2,0))\n",
        "\n",
        "    # Visualization for Model 1\n",
        "    _ = viz.visualize_image_attr(np.transpose(attr1.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                                 original_image_np, method='blended_heat_map', sign='all',\n",
        "                                 show_colorbar=True, title=f\"Swin-T Prediction: {pred1_label}\")\n",
        "\n",
        "    # Visualization for Model 2\n",
        "    _ = viz.visualize_image_attr(np.transpose(attr2.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                                 original_image_np, method='blended_heat_map', sign='all',\n",
        "                                 show_colorbar=True, title=f\"ConvNeXt Prediction: {pred2_label}\")\n",
        "else:\n",
        "    print(\"No disagreement cases found in the last fold to visualize. The models were in perfect agreement!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zvEmHhO6nKMf",
        "outputId": "815a518f-1ebd-4e68-f2e2-6b8d9b67c06c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Configuring Environment for Dual-Architecture Framework ---\n",
            "Using device: cuda\n",
            "--- 2. Defining Advanced Data Augmentations ---\n",
            "--- 3. Loading Models: 'swin_tiny_patch4_window7_224' and 'convnext_tiny.in12k_ft_in1k' ---\n",
            "\n",
            "--- 5. Starting Stratified 10-Fold Cross-Validation ---\n",
            "\n",
            "===== FOLD 1/10 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dual Features: 100%|██████████| 85/85 [00:32<00:00,  2.61it/s]\n",
            "Extracting Dual Features: 100%|██████████| 10/10 [00:05<00:00,  1.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SVM for Swin Transformer...\n",
            "Training SVM for ConvNeXt...\n",
            "Fold 1 Final Accuracy: 0.9900\n",
            "Fold 1 Disagreement Rate: 0.0100 (3 cases flagged)\n",
            "\n",
            "===== FOLD 2/10 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dual Features: 100%|██████████| 85/85 [00:39<00:00,  2.16it/s]\n",
            "Extracting Dual Features: 100%|██████████| 10/10 [00:03<00:00,  2.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SVM for Swin Transformer...\n",
            "Training SVM for ConvNeXt...\n",
            "Fold 2 Final Accuracy: 0.9900\n",
            "Fold 2 Disagreement Rate: 0.0233 (7 cases flagged)\n",
            "\n",
            "===== FOLD 3/10 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dual Features: 100%|██████████| 85/85 [00:31<00:00,  2.71it/s]\n",
            "Extracting Dual Features: 100%|██████████| 10/10 [00:03<00:00,  2.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SVM for Swin Transformer...\n",
            "Training SVM for ConvNeXt...\n",
            "Fold 3 Final Accuracy: 0.9967\n",
            "Fold 3 Disagreement Rate: 0.0067 (2 cases flagged)\n",
            "\n",
            "===== FOLD 4/10 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dual Features: 100%|██████████| 85/85 [00:31<00:00,  2.72it/s]\n",
            "Extracting Dual Features: 100%|██████████| 10/10 [00:03<00:00,  2.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SVM for Swin Transformer...\n",
            "Training SVM for ConvNeXt...\n",
            "Fold 4 Final Accuracy: 0.9833\n",
            "Fold 4 Disagreement Rate: 0.0167 (5 cases flagged)\n",
            "\n",
            "===== FOLD 5/10 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dual Features: 100%|██████████| 85/85 [00:31<00:00,  2.70it/s]\n",
            "Extracting Dual Features: 100%|██████████| 10/10 [00:03<00:00,  2.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SVM for Swin Transformer...\n",
            "Training SVM for ConvNeXt...\n",
            "Fold 5 Final Accuracy: 0.9933\n",
            "Fold 5 Disagreement Rate: 0.0100 (3 cases flagged)\n",
            "\n",
            "===== FOLD 6/10 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dual Features: 100%|██████████| 85/85 [00:31<00:00,  2.67it/s]\n",
            "Extracting Dual Features: 100%|██████████| 10/10 [00:03<00:00,  2.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SVM for Swin Transformer...\n",
            "Training SVM for ConvNeXt...\n",
            "Fold 6 Final Accuracy: 0.9767\n",
            "Fold 6 Disagreement Rate: 0.0200 (6 cases flagged)\n",
            "\n",
            "===== FOLD 7/10 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dual Features: 100%|██████████| 85/85 [00:34<00:00,  2.50it/s]\n",
            "Extracting Dual Features: 100%|██████████| 10/10 [00:03<00:00,  2.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SVM for Swin Transformer...\n",
            "Training SVM for ConvNeXt...\n",
            "Fold 7 Final Accuracy: 0.9900\n",
            "Fold 7 Disagreement Rate: 0.0100 (3 cases flagged)\n",
            "\n",
            "===== FOLD 8/10 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dual Features: 100%|██████████| 85/85 [00:31<00:00,  2.66it/s]\n",
            "Extracting Dual Features: 100%|██████████| 10/10 [00:03<00:00,  2.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SVM for Swin Transformer...\n",
            "Training SVM for ConvNeXt...\n",
            "Fold 8 Final Accuracy: 0.9967\n",
            "Fold 8 Disagreement Rate: 0.0067 (2 cases flagged)\n",
            "\n",
            "===== FOLD 9/10 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dual Features: 100%|██████████| 85/85 [00:31<00:00,  2.73it/s]\n",
            "Extracting Dual Features: 100%|██████████| 10/10 [00:03<00:00,  2.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SVM for Swin Transformer...\n",
            "Training SVM for ConvNeXt...\n",
            "Fold 9 Final Accuracy: 0.9933\n",
            "Fold 9 Disagreement Rate: 0.0100 (3 cases flagged)\n",
            "\n",
            "===== FOLD 10/10 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dual Features: 100%|██████████| 85/85 [00:33<00:00,  2.50it/s]\n",
            "Extracting Dual Features: 100%|██████████| 10/10 [00:03<00:00,  2.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SVM for Swin Transformer...\n",
            "Training SVM for ConvNeXt...\n",
            "Fold 10 Final Accuracy: 0.9900\n",
            "Fold 10 Disagreement Rate: 0.0067 (2 cases flagged)\n",
            "\n",
            "--- 6. Final Dual-Architecture Framework Performance Report ---\n",
            "Average Accuracy:      0.9900 ± 0.0058\n",
            "Average Sensitivity:   0.9900 ± 0.0080\n",
            "Average Specificity:   0.9900 ± 0.0075\n",
            "Average Disagreement Rate: 1.20% (Flagged for human review)\n",
            "\n",
            "--- 7. Generating Comparative XAI for a Disagreement Case ---\n",
            "Analyzing a disagreement case. True Label: 'no'\n",
            "Swin Transformer Predicted: 'no'\n",
            "ConvNeXt Predicted: 'yes'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "cannot assign module before Module.__init__() call",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-2294181395.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# Prepare models and layers for Grad-CAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# For Swin-T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mmodel1_wrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ModelWrapper'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'forward'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__init__'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0mlayer1_gc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLayerGradCam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2-2294181395.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# Prepare models and layers for Grad-CAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# For Swin-T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mmodel1_wrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ModelWrapper'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'forward'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__init__'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0mlayer1_gc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLayerGradCam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1964\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmodules\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1965\u001b[0;31m                     raise AttributeError(\n\u001b[0m\u001b[1;32m   1966\u001b[0m                         \u001b[0;34m\"cannot assign module before Module.__init__() call\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1967\u001b[0m                     )\n",
            "\u001b[0;31mAttributeError\u001b[0m: cannot assign module before Module.__init__() call"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "   import captum\n",
        "   print(captum.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoTTpHj_nU1n",
        "outputId": "c5fb3ff3-e1a4-4e57-9d3f-d944a797ad73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision scikit-learn tqdm Pillow timm\n",
        "!pip install peft transformers\n",
        "!pip install opencv-python scikit-image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjRAGkHazLSn",
        "outputId": "3329e6cd-a4b3-427f-e0ab-7c12c4ccbcb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.18)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.33.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.7.14)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.9.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.33.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.16.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.6.11)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from skimage.filters import threshold_otsu\n",
        "from transformers import ViTForImageClassification, ViTConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# Import additional metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# --- 1. Neuro-Aware Preprocessing Pipeline ---\n",
        "print(\"--- 1. Defining Neuro-Aware Preprocessing ---\")\n",
        "\n",
        "class NeuroAwareTransform:\n",
        "    \"\"\"\n",
        "    A custom transform pipeline for brain MRIs.\n",
        "    1. Converts to grayscale for skull stripping.\n",
        "    2. Applies Otsu's thresholding to create a mask (simple skull stripping).\n",
        "    3. Applies the mask to the original color image.\n",
        "    4. Applies CLAHE for contrast enhancement on the focused brain tissue.\n",
        "    \"\"\"\n",
        "    def __init__(self, output_size=224):\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # Convert PIL image to OpenCV format\n",
        "        img_cv = np.array(img)\n",
        "        img_rgb = cv2.cvtColor(img_cv, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        # 1. Grayscale for masking\n",
        "        gray = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # 2. Otsu's Thresholding for a simple brain mask\n",
        "        thresh = threshold_otsu(gray)\n",
        "        mask = gray > thresh\n",
        "\n",
        "        # 3. Apply mask to original RGB image\n",
        "        masked_img = img_rgb * mask[:, :, np.newaxis].astype(img_rgb.dtype)\n",
        "\n",
        "        # 4. Apply CLAHE to the masked image's LAB channels for contrast\n",
        "        lab = cv2.cvtColor(masked_img, cv2.COLOR_BGR2LAB)\n",
        "        l, a, b = cv2.split(lab)\n",
        "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
        "        cl = clahe.apply(l)\n",
        "        limg = cv2.merge((cl,a,b))\n",
        "        final_img_bgr = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "        # Convert back to PIL Image for torchvision transforms\n",
        "        final_img_pil = Image.fromarray(cv2.cvtColor(final_img_bgr, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        return final_img_pil\n",
        "\n",
        "# Define the full pipeline\n",
        "data_transform = transforms.Compose([\n",
        "    NeuroAwareTransform(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# --- 2. The Novel Gated Attention Head ---\n",
        "print(\"--- 2. Defining the Gated Attention MLP Head ---\")\n",
        "\n",
        "class GatedAttentionMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # The gating layer\n",
        "        self.gate_fc = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.silu = nn.SiLU() # Swish activation, works well with gates\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get values and gates\n",
        "        values = self.silu(self.fc1(x))\n",
        "        gates = torch.sigmoid(self.gate_fc(x))\n",
        "\n",
        "        # Element-wise multiplication (gating)\n",
        "        gated_output = values * gates\n",
        "\n",
        "        gated_output = self.dropout(gated_output)\n",
        "        return self.fc2(gated_output)\n",
        "\n",
        "# --- 3. The Complete NeuroViT Model ---\n",
        "print(\"--- 3. Defining the End-to-End NeuroViT Model ---\")\n",
        "\n",
        "class NeuroViT(nn.Module):\n",
        "    def __init__(self, model_name='google/vit-base-patch16-224-in21k', num_classes=2, k_top_patches=10):\n",
        "        super().__init__()\n",
        "        self.k_top_patches = k_top_patches\n",
        "\n",
        "        # Load ViT config and modify it to output attentions\n",
        "        config = ViTConfig.from_pretrained(model_name)\n",
        "        config.output_attentions = True\n",
        "        self.vit = ViTForImageClassification.from_pretrained(model_name, config=config, ignore_mismatched_sizes=True)\n",
        "\n",
        "        self.hidden_dim = config.hidden_size\n",
        "\n",
        "        # Novelty: Fusion dimension is the global CLS token + K flattened patch tokens\n",
        "        fusion_dim = self.hidden_dim + (self.hidden_dim * self.k_top_patches)\n",
        "\n",
        "        # Replace the classifier with our new Gated Head\n",
        "        self.custom_head = GatedAttentionMLP(fusion_dim, self.hidden_dim // 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get ViT outputs\n",
        "        outputs = self.vit.vit(x)\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "        attentions = outputs.attentions[-1] # Attentions from the last layer\n",
        "\n",
        "        # --- Attention-Based Feature Selection ---\n",
        "        # 1. CLS Token (Global representation)\n",
        "        cls_token = last_hidden_state[:, 0, :]\n",
        "\n",
        "        # 2. Patch Tokens\n",
        "        patch_tokens = last_hidden_state[:, 1:, :]\n",
        "\n",
        "        # 3. Calculate importance score for each patch from attention heads\n",
        "        # We average the attention given to the CLS token from all patch tokens\n",
        "        # across all attention heads as a proxy for patch importance.\n",
        "        attn_scores = attentions[:, :, 0, 1:].mean(dim=1) # Shape: (batch_size, num_patches)\n",
        "\n",
        "        # 4. Find the top-k most important patches\n",
        "        top_k_scores, top_k_indices = torch.topk(attn_scores, self.k_top_patches, dim=1)\n",
        "\n",
        "        # 5. Gather the corresponding patch tokens\n",
        "        # We need to expand indices to match the hidden dimension\n",
        "        top_k_indices = top_k_indices.unsqueeze(-1).expand(-1, -1, self.hidden_dim)\n",
        "        top_patch_tokens = torch.gather(patch_tokens, 1, top_k_indices)\n",
        "\n",
        "        # --- Global-Local Fusion ---\n",
        "        # Flatten the top patch tokens\n",
        "        flat_top_patch_tokens = top_patch_tokens.flatten(start_dim=1)\n",
        "\n",
        "        # Concatenate CLS token with the important patch tokens\n",
        "        fused_features = torch.cat([cls_token, flat_top_patch_tokens], dim=1)\n",
        "\n",
        "        # --- Final Classification ---\n",
        "        logits = self.custom_head(fused_features)\n",
        "\n",
        "        # Return logits and the attention map for explainability\n",
        "        return logits, attn_scores\n",
        "\n",
        "\n",
        "# --- 4. Configuration and Training Setup ---\n",
        "print(\"--- 4. Configuring Training Environment and PEFT (LoRA) ---\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_DIR = \"/content/sample_data/Train\"\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 16 # Use a smaller batch size due to attention computations\n",
        "EPOCHS = 10\n",
        "\n",
        "# Load dataset\n",
        "dataset = datasets.ImageFolder(root=DATA_DIR, transform=data_transform)\n",
        "train_size = int(0.85 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_data, val_data = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "class_names = dataset.classes\n",
        "\n",
        "# Initialize the NeuroViT model\n",
        "model = NeuroViT(num_classes=len(class_names)).to(device)\n",
        "\n",
        "# --- LoRA Configuration ---\n",
        "# This is the key to efficient fine-tuning. We only train the LoRA weights and our new custom head.\n",
        "lora_config = LoraConfig(\n",
        "    r=16, # Rank of the update matrices. Higher means more parameters.\n",
        "    lora_alpha=32, # Alpha scaling factor.\n",
        "    target_modules=[\"query\", \"value\"], # Apply LoRA to query and value matrices in attention blocks\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    # THE FIX: Use SEQ_CLS for Vision Transformer classification tasks\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        ")\n",
        "\n",
        "# Make the ViT part of our model tunable with LoRA\n",
        "model.vit = get_peft_model(model.vit, lora_config)\n",
        "model.vit.print_trainable_parameters()\n",
        "\n",
        "# Optimizer and Loss\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# --- 5. Training and Validation Loop ---\n",
        "print(\"\\n--- 5. Starting End-to-End Training of NeuroViT ---\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss, total_correct = 0, 0\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, _ = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_correct += (logits.argmax(dim=1) == labels).sum().item()\n",
        "\n",
        "    # Validation (per epoch)\n",
        "    model.eval()\n",
        "    val_loss, val_correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            logits, _ = model(images)\n",
        "            val_loss += criterion(logits, labels).item()\n",
        "            val_correct += (logits.argmax(dim=1) == labels).sum().item()\n",
        "\n",
        "    print(f\"Train Acc: {total_correct/len(train_data):.4f} | Val Acc: {val_correct/len(val_data):.4f}\")\n",
        "\n",
        "# --- 6. Final Model Evaluation ---\n",
        "print(\"\\n--- 6. Performing Final Model Evaluation on Validation Set ---\")\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(val_loader, desc=\"Final Evaluation\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        logits, _ = model(images)\n",
        "        predictions = logits.argmax(dim=1)\n",
        "\n",
        "        all_preds.extend(predictions.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate and print comprehensive metrics\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "print(cm)\n",
        "\n",
        "# You can also print individual metrics if desired\n",
        "final_accuracy = accuracy_score(all_labels, all_preds)\n",
        "final_precision = precision_score(all_labels, all_preds, average='weighted') # Use weighted for multi-class\n",
        "final_recall = recall_score(all_labels, all_preds, average='weighted') # Use weighted for multi-class\n",
        "final_f1 = f1_score(all_labels, all_preds, average='weighted') # Use weighted for multi-class\n",
        "\n",
        "print(f\"\\nFinal Accuracy: {final_accuracy:.4f}\")\n",
        "print(f\"Final Precision (weighted): {final_precision:.4f}\")\n",
        "print(f\"Final Recall (weighted): {final_recall:.4f}\")\n",
        "print(f\"Final F1-Score (weighted): {final_f1:.4f}\")\n",
        "\n",
        "\n",
        "# --- 7. Explainability: Visualize Attention on a Test Image ---\n",
        "# (Renumbered from 6 to 7)\n",
        "print(\"\\n--- 7. Generating Explainability Overlay ---\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Get a sample image from the validation set\n",
        "    # Ensure val_data is accessible and has at least one element\n",
        "    if len(val_data) > 0:\n",
        "        img_tensor, label = val_data[0]\n",
        "        img_tensor = img_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # Get the prediction and the attention map\n",
        "        logits, attn_map = model(img_tensor)\n",
        "        pred_label = class_names[logits.argmax(dim=1).item()]\n",
        "\n",
        "        # Reshape attention map to a 2D grid (14x14 for ViT-B16)\n",
        "        # Assuming a 224x224 input and 16x16 patches, num_patches_side will be 14\n",
        "        num_patches_side = int(np.sqrt(attn_map.shape[1]))\n",
        "        attn_heatmap = attn_map.reshape((num_patches_side, num_patches_side)).cpu().numpy()\n",
        "\n",
        "        # Upscale heatmap to image size and overlay\n",
        "        heatmap_resized = cv2.resize(attn_heatmap, (224, 224))\n",
        "        heatmap_normalized = (heatmap_resized - heatmap_resized.min()) / (heatmap_resized.max() - heatmap_normalized.min())\n",
        "        heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap_normalized), cv2.COLORMAP_JET)\n",
        "\n",
        "        # Superimpose heatmap on original image\n",
        "        original_img_np = np.transpose(val_data[0][0].numpy(), (1, 2, 0))\n",
        "        # Un-normalize for visualization (assuming mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
        "        original_img_np = (original_img_np * 0.5) + 0.5\n",
        "        original_img_np = np.uint8(255 * original_img_np)\n",
        "\n",
        "        superimposed_img = cv2.addWeighted(cv2.cvtColor(original_img_np, cv2.COLOR_RGB2BGR), 0.6, heatmap_colored, 0.4, 0)\n",
        "\n",
        "        # Display using Pillow\n",
        "        print(f\"Sample Image - True Label: {class_names[label]}, Predicted: {pred_label}\")\n",
        "        from IPython.display import display # Ensure display is imported for Colab/Jupyter\n",
        "        display(Image.fromarray(cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB)))\n",
        "    else:\n",
        "        print(\"Validation data is empty. Cannot generate explainability overlay.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MHqOB6fk0b8u",
        "outputId": "9e83e384-a0f4-4249-8acd-4f43219e5eed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Defining Neuro-Aware Preprocessing ---\n",
            "--- 2. Defining the Gated Attention MLP Head ---\n",
            "--- 3. Defining the End-to-End NeuroViT Model ---\n",
            "--- 4. Configuring Training Environment and PEFT (LoRA) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 591,362 || all params: 86,391,556 || trainable%: 0.6845\n",
            "\n",
            "--- 5. Starting End-to-End Training of NeuroViT ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 160/160 [01:30<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 0.8631 | Val Acc: 0.9689\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 160/160 [01:28<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 0.9835 | Val Acc: 0.9867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 160/160 [01:26<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 0.9922 | Val Acc: 0.9889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 160/160 [01:27<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 0.9988 | Val Acc: 0.9489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 160/160 [01:28<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 0.9906 | Val Acc: 0.9911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 160/160 [01:27<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 0.9996 | Val Acc: 0.9956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 160/160 [01:27<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 1.0000 | Val Acc: 0.9956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 160/160 [01:27<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 1.0000 | Val Acc: 0.9956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 160/160 [01:27<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 1.0000 | Val Acc: 0.9956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 160/160 [01:27<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 1.0000 | Val Acc: 0.9956\n",
            "\n",
            "--- 6. Performing Final Model Evaluation on Validation Set ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Final Evaluation: 100%|██████████| 29/29 [00:09<00:00,  2.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Classification Report ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          no       0.99      1.00      1.00       241\n",
            "         yes       1.00      0.99      1.00       209\n",
            "\n",
            "    accuracy                           1.00       450\n",
            "   macro avg       1.00      1.00      1.00       450\n",
            "weighted avg       1.00      1.00      1.00       450\n",
            "\n",
            "\n",
            "--- Confusion Matrix ---\n",
            "[[241   0]\n",
            " [  2 207]]\n",
            "\n",
            "Final Accuracy: 0.9956\n",
            "Final Precision (weighted): 0.9956\n",
            "Final Recall (weighted): 0.9956\n",
            "Final F1-Score (weighted): 0.9956\n",
            "\n",
            "--- 7. Generating Explainability Overlay ---\n",
            "Sample Image - True Label: no, Predicted: no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=224x224>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}